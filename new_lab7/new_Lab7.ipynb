{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 6s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01176471\n",
      " 0.02745098 0.02745098 0.01568628 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01568628\n",
      " 0.05098039 0.47058824 0.5372549  0.7294118  0.99607843 0.99607843\n",
      " 0.53333336 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.20784314 0.56078434 0.7019608  0.99607843 0.99607843\n",
      " 0.99607843 1.         0.99607843 0.99607843 0.75686276 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01568628 0.09803922 0.19607843 0.58431375 0.6862745  0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843 1.\n",
      " 0.99607843 0.99607843 0.42352942 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3764706  0.6666667  0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.99607843 0.9607843  0.7176471\n",
      " 0.3882353  0.3882353  0.5764706  0.9882353  0.99607843 0.76862746\n",
      " 0.01176471 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01568628 0.5686275  0.63529414\n",
      " 0.63529414 0.85882354 0.99607843 0.99607843 0.99607843 0.9647059\n",
      " 0.7294118  0.3647059  0.25882354 0.         0.         0.\n",
      " 0.49019608 0.9647059  0.99607843 0.46666667 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.34117648 0.84313726 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.8        0.34117648 0.26666668 0.         0.\n",
      " 0.         0.         0.         0.18039216 0.77254903 0.99607843\n",
      " 0.7058824  0.18431373 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.65882355\n",
      " 0.99607843 0.6117647  0.3137255  0.3137255  0.3137255  0.10980392\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.49411765 0.9607843  1.         0.22352941 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00784314 0.2901961  0.12941177\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.16862746 0.78431374\n",
      " 0.99607843 0.35686275 0.02352941 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.6117647  0.9607843  0.8039216  0.05098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.4862745\n",
      " 0.95686275 0.99607843 0.33333334 0.02352941 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.23137255 0.8509804  0.99607843 0.4745098\n",
      " 0.02745098 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.6901961  0.99607843 0.99607843 0.23921569 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.6313726  0.9607843  0.84313726\n",
      " 0.22745098 0.02745098 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21176471 0.9647059  0.99607843 0.7254902  0.02745098 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.4862745  0.99607843\n",
      " 0.99607843 0.2509804  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.20392157 0.92941177 0.99607843 0.49411765 0.01960784\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.4627451\n",
      " 0.99607843 0.99607843 0.46666667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.4627451  0.99607843 0.99607843\n",
      " 0.23921569 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00784314 0.02352941 0.02352941 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[101])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(784,))\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.add(Dense(64, activation = 'relu',input_shape =(784,)))\n",
    "model.add(Dense(nb_classes,activation = \"sigmoid\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.4180 - acc: 0.8859 - val_loss: 0.2133 - val_acc: 0.9369\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.1868 - acc: 0.9456 - val_loss: 0.1607 - val_acc: 0.9543\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.1421 - acc: 0.9587 - val_loss: 0.1415 - val_acc: 0.9576\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.1177 - acc: 0.9653 - val_loss: 0.1140 - val_acc: 0.9666\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0973 - acc: 0.9710 - val_loss: 0.1086 - val_acc: 0.9687\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0864 - acc: 0.9741 - val_loss: 0.1094 - val_acc: 0.9678\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0758 - acc: 0.9773 - val_loss: 0.0988 - val_acc: 0.9709\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0661 - acc: 0.9804 - val_loss: 0.1013 - val_acc: 0.9712\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0573 - acc: 0.9823 - val_loss: 0.1041 - val_acc: 0.9706\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0515 - acc: 0.9839 - val_loss: 0.0933 - val_acc: 0.9744\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0458 - acc: 0.9856 - val_loss: 0.0930 - val_acc: 0.9762\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0418 - acc: 0.9871 - val_loss: 0.0995 - val_acc: 0.9732\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0372 - acc: 0.9884 - val_loss: 0.0973 - val_acc: 0.9736\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0328 - acc: 0.9898 - val_loss: 0.1138 - val_acc: 0.9686\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0288 - acc: 0.9912 - val_loss: 0.1046 - val_acc: 0.9742\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0269 - acc: 0.9914 - val_loss: 0.0995 - val_acc: 0.9723\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0235 - acc: 0.9927 - val_loss: 0.1035 - val_acc: 0.9736\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0210 - acc: 0.9938 - val_loss: 0.0992 - val_acc: 0.9745\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.1211 - val_acc: 0.9713\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0180 - acc: 0.9945 - val_loss: 0.1171 - val_acc: 0.9730\n",
      "Test score: [0.11710637128894305, 0.973]\n",
      "Test accuracy: [0.11710637128894305, 0.973]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layer = 6\n",
    "model1 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(Dense(61,activation = \"relu\", input_shape = (784,)))\n",
    "for k in range(n_hidden_layer):\n",
    "    model1.add(Dense(64,activation = \"relu\"))\n",
    "    model1.add(Dropout(0.2))\n",
    "model1.add(Dense(nb_classes, activation = 'softmax',input_shape =(784,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.8941 - acc: 0.6809 - val_loss: 0.2830 - val_acc: 0.9215\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.3139 - acc: 0.9207 - val_loss: 0.2272 - val_acc: 0.9417\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.2315 - acc: 0.9434 - val_loss: 0.1745 - val_acc: 0.9548\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.1921 - acc: 0.9534 - val_loss: 0.1688 - val_acc: 0.9584\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.1694 - acc: 0.9585 - val_loss: 0.1424 - val_acc: 0.9652\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.1514 - acc: 0.9639 - val_loss: 0.1369 - val_acc: 0.9655\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.1338 - acc: 0.9679 - val_loss: 0.1509 - val_acc: 0.9637\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.1256 - acc: 0.9688 - val_loss: 0.1663 - val_acc: 0.9636\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.1201 - acc: 0.9720 - val_loss: 0.1419 - val_acc: 0.9661\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.1062 - acc: 0.9740 - val_loss: 0.1221 - val_acc: 0.9716\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.0994 - acc: 0.9750 - val_loss: 0.1570 - val_acc: 0.9638\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0929 - acc: 0.9771 - val_loss: 0.1221 - val_acc: 0.9731\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0893 - acc: 0.9782 - val_loss: 0.1281 - val_acc: 0.9726\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0848 - acc: 0.9792 - val_loss: 0.1388 - val_acc: 0.9718\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.0809 - acc: 0.9800 - val_loss: 0.1382 - val_acc: 0.9719\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0801 - acc: 0.9799 - val_loss: 0.1321 - val_acc: 0.9713\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.0730 - acc: 0.9818 - val_loss: 0.1231 - val_acc: 0.9732\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0670 - acc: 0.9837 - val_loss: 0.1337 - val_acc: 0.9712\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0663 - acc: 0.9835 - val_loss: 0.1316 - val_acc: 0.9721\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.0660 - acc: 0.9838 - val_loss: 0.1312 - val_acc: 0.9736\n",
      "Test score: [0.13124630682555727, 0.9736]\n",
      "Test accuracy: [0.13124630682555727, 0.9736]\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model1.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
